{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import tarfile\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd \n",
    "from pycocotools.coco import COCO\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from typing import Optional\n",
    "from typing import Callable\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=17.62s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training images: 27358\n",
      "Number of validation images: 1160\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = Path(\"/home/ubuntu/442/COCO_data/\") \n",
    "train_annotations = COCO(ROOT_PATH / \"annotations/instances_train2017.json\")\n",
    "valid_annotations = COCO(ROOT_PATH / \"annotations/instances_val2017.json\")\n",
    "\n",
    "cat_ids = train_annotations.getCatIds(supNms=[\"vehicle\"])\n",
    "train_img_ids = []\n",
    "for cat in cat_ids:\n",
    "    train_img_ids.extend(train_annotations.getImgIds(catIds=cat))\n",
    "    \n",
    "train_img_ids = list(set(train_img_ids))\n",
    "print(f\"Number of training images: {len(train_img_ids)}\")\n",
    "\n",
    "valid_img_ids = []\n",
    "for cat in cat_ids:\n",
    "    valid_img_ids.extend(valid_annotations.getImgIds(catIds=cat))\n",
    "    \n",
    "valid_img_ids = list(set(valid_img_ids))\n",
    "print(f\"Number of validation images: {len(valid_img_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageData(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        annotations: COCO, \n",
    "        img_ids: List[int], \n",
    "        cat_ids: List[int], \n",
    "        root_path: Path, \n",
    "        transform: Optional[Callable]=None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.annotations = annotations\n",
    "        self.img_data = annotations.loadImgs(img_ids)\n",
    "        self.cat_ids = cat_ids\n",
    "        self.files = [str(root_path / img[\"file_name\"]) for img in self.img_data]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.LongTensor]:\n",
    "        ann_ids = self.annotations.getAnnIds(\n",
    "            imgIds=self.img_data[i]['id'], \n",
    "            catIds=self.cat_ids, \n",
    "            iscrowd=None\n",
    "        )\n",
    "        anns = self.annotations.loadAnns(ann_ids)\n",
    "        mask = torch.LongTensor(np.max(np.stack([self.annotations.annToMask(ann) * ann[\"category_id\"] \n",
    "                                                 for ann in anns]), axis=0)).unsqueeze(0)\n",
    "        \n",
    "        img = torchvision.io.read_image(self.files[i])\n",
    "        if img.shape[0] == 1:\n",
    "            img = torch.cat([img]*3)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            return self.transform(img, mask)\n",
    "        \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transform(\n",
    "    img1: torch.LongTensor, \n",
    "    img2: torch.LongTensor\n",
    ") -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    params = transforms.RandomResizedCrop.get_params(img1, scale=(0.5, 1.0), ratio=(0.75, 1.33))\n",
    "    \n",
    "    IMAGE_SIZE = [512,512]\n",
    "    img1 = TF.resized_crop(img1, *params, size=IMAGE_SIZE)\n",
    "    img2 = TF.resized_crop(img2, *params, size=IMAGE_SIZE)\n",
    "    \n",
    "    # Random horizontal flipping\n",
    "    if random.random() > 0.5:\n",
    "        img1 = TF.hflip(img1)\n",
    "        img2 = TF.hflip(img2)\n",
    "        \n",
    "    return img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH_IMG = ROOT_PATH = Path(\"/home/ubuntu/442/COCO_data/coco_img\") \n",
    "train_data = ImageData(train_annotations, train_img_ids, cat_ids, ROOT_PATH_IMG / \"train2017\", train_transform)\n",
    "valid_data = ImageData(valid_annotations, valid_img_ids, cat_ids, ROOT_PATH_IMG / \"val2017\", train_transform)\n",
    "BATCH_SIZE = 16\n",
    "train_dl = DataLoader(\n",
    "    train_data,\n",
    "    BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    drop_last=True, \n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    valid_data,\n",
    "    BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    drop_last=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8997204df8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m122\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "img, mask = train_data[22]\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(TF.to_pil_image(img))\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask.squeeze())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
